---
title: "p8105_hw3_xz3078"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
```

## Problem 1

```{r}
library(p8105.datasets)
data("instacart")
```

This dataset has `r nrow(instacart)` rows and `r ncol(instacart)` columns with variable names `r colnames(instacart)`. Among these variables, "add_to_cart_order" is order in which each product was added to cart; "reordered" is a dummy variable equals 1 if this prodcut has been ordered by this user in the past, and 0 otherwise; "order_number" is the order sequence number for this user; "order_dow" is the day of the week on which the order was placed; "order_hour_of_day" is the hour of the day on which the order was placed; "days_since_prior_order" is days since the last order, capped at 30, NA if order_number=1. 

For example, the first row in "instacart" means the order id is 1; the product id is 49302; it's the first item added to the cart; it has been ordered by this user before; the user id is 112108; this order belongs in "train" evaluation set; the order sequence number for this user is 4; the order is placed on Thursday 10am; 9 days since the last order; the product is Bulgarian Yogurt; the aisle is yogurt with id 120; the department is dairy eggs with id 16. 

```{r}
instacart %>%
  summarize(
    n_aisle = n_distinct(aisle))

instacart %>%
  group_by(aisle) %>%
  summarize(
    n_obs = n()) %>%
  arrange(desc(n_obs))
```

There are 134 aisles, and fresh vegetables is the most items ordered from. 

```{r}
instacart %>%
  group_by(aisle) %>%
  summarize(
    n_obs = n()) %>%
  filter(n_obs > 10000) %>%
  ggplot(aes(x = reorder(aisle, -n_obs), y= n_obs)) + 
    geom_point() +
  labs(y = "number of items ordered",
       x = "aisle") +
  theme(axis.text.x = element_text(angle=90, hjust=1))
```

There are 39 aisles with more than 10000 items ordered. The aisles with the most items ordered are fresh vegetables, fresh fruits, and packaged vegetables fruits. 

```{r}
instacart %>%
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>%
  count(product_name, name = "n_product") %>%
  slice_max(n_product, n=3) %>%
  knitr::kable()
```

The three most popular items in “baking ingredients” are "Light Brown Sugar" with 499 items, "Pure Baking Soda" 387 items, and "Cane Sugar" with 336 items. The three most popular items in “dog food care” are "Snack Sticks Chicken & Rice Recipe Dog Treats" with 30 items, "Organix Chicken & Brown Rice Recipe" with 28 items, and "Small Dog Biscuits" with 26 items. The three most popular items in “packaged vegetables fruits” are "Organic Baby Spinach" 9784 items, "Organic Raspberries" 5546 items, and "Organic Blueberries" with 4966 items.

```{r}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  select(product_name, order_dow, order_hour_of_day) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hod = mean(order_hour_of_day)) %>%
  pivot_wider(
    names_from = "order_dow", 
    values_from = "mean_hod"
  ) %>%
  rename(Sunday = "0", Monday = "1", Tuesday = "2", Wednesday = "3", Thursday = "4", Friday = "5", Saturday = "6") %>%
  knitr::kable()
```

The mean hour of the day at which Coffee Ice Cream are ordered from Sunday to Saturday are 13.8, 14.3, 15.4, 15.3, 15.2, 12.3, and 13.8. The mean hour of the day at which Pink Lady Apples are ordered from Sunday to Saturday are 13.4, 11.4, 11.7, 14.3, 11.6, 12.8, and 11.9.

## Problem 2

```{r}
library(p8105.datasets)
data("brfss_smart2010")
```

```{r}
brfss = 
  brfss_smart2010 %>%
  janitor::clean_names() %>% 
  rename(state = locationabbr, location = locationdesc) %>%
  filter(topic == "Overall Health") %>%
  filter(response %in% c("Poor", "Fair", "Good", "Very good", "Excellent")) %>%
  mutate(response = ordered(response, levels = c("Poor", "Fair", "Good", "Very good", "Excellent"))) 
```

```{r}
brfss %>%
  filter(year == 2002) %>%
  group_by(state) %>%
  summarize(
    n_location = n_distinct(location)) %>%
  filter(n_location >= 7)

brfss %>%
  filter(year == 2010) %>%
  group_by(state) %>%
  summarize(
    n_location = n_distinct(location)) %>%
  filter(n_location >= 7)
```

In 2002, CT, FL, MA, NC, NJ, PA were observed at 7 or more locations. In 2010, CA, CO, FL, MA, MD, NC, NE, NJ, NY, OH, PA, SC, TX, WA were observed at 7 or more locations.

```{r}
brfss %>%
  filter(response == "Excellent") %>%
  group_by(state, year) %>%
  mutate(mean_value = mean(data_value, na.rm = TRUE)) %>%
  select(year, state, mean_value) %>%
  distinct() %>%
  ggplot(aes(x = year, y = mean_value, color = state)) + 
    geom_line(alpha = .5) + 
    labs(title = "“spaghetti” plot of average value over time",
         y = "mean value")
```

We can see from the plot that there is a downward trend for the mean data value for all states. 

```{r}
brfss %>%
  filter(year %in% c(2006, 2010), state == "NY") %>%
  group_by(year, response) %>%
  drop_na(data_value) %>%
  ggplot(aes(x = data_value, fill = response)) + 
    geom_density(alpha = .5) +
    labs(title = "distribution of data value for responses among locations in NY State") + 
    facet_grid(. ~ year)
```

We can see from the plot that the data value in 2010 is better than 2006 in general. The value for "Excellent" response is between "Fair" and "Good". 

## Problem 3

```{r message = FALSE}
accel=
  read_csv("./data/accel_data.csv") %>%
  janitor::clean_names() %>%
  mutate(weekend = ifelse(day %in% c("Saturday", "Sunday"), 1, 0)) %>%
  select(week, day_id, day, weekend, everything())

accel
```

In this dataset, there are `r nrow(accel)` observations and `r ncol(accel)` variables. Variables "activity_*" are the activity counts for each minute of a 24-hour day starting at midnight. There are 5 weeks, variable "weekend" equals to 1 if it's Saturday or Sunday, equals to 0 if it's weekdays. 

```{r}
accel %>%
  mutate(activity_total = select(., activity_1:activity_1440) %>% rowSums(na.rm = TRUE)) %>%
  select(week, day_id, day, weekend, activity_total) %>%
  knitr::kable()
```

We can see from the table that for week 1 and 2, the weekend activity total are much higher than weekdays. In week 3, Monday has the highest activity total. In week 4 and 5, the activity total at weekend are less than weekdays, and he barely use accelerometers on Saturday. In general, the total activity has a decrease trend as week increases. 

```{r}
accel %>%
  pivot_longer(
    activity_1:activity_1440,
    names_to = "activity_minute", 
    values_to = "activity"
  ) %>%
  separate(activity_minute, into = c("act", "minute"), sep = 9) %>%
  mutate(minute = as.numeric(minute)) %>%
  ggplot(aes(x = minute, y = activity, color = day)) + 
    geom_line() + 
    labs(title = "24-hour activity time courses for each day") +
    scale_x_continuous(breaks=seq(0, 1440, 60)) + 
    theme(axis.text.x = element_text(angle=90, hjust=1))
```

We can see from the graph that from 7pm to 11pm is the peak of using accelerometer data for all day of week. Another small peak is from 11am to 12pm for Sunday. From 11pm to 5am is the minimum of using accelerometer data. These all make sense because 11pm to 5am is time for sleep and people usually use more accelerometer data before bed time and at noon. 



